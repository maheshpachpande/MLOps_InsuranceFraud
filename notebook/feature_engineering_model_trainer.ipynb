{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import (\n",
        "    FunctionTransformer,\n",
        "    KBinsDiscretizer,\n",
        "    StandardScaler,\n",
        "    OneHotEncoder,\n",
        "    OrdinalEncoder,\n",
        ")\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ========== Logging Setup ==========\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n"
      ],
      "metadata": {
        "id": "W0CfuHtx-XpV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Configuration ==========\n",
        "CONFIG = {\n",
        "    \"data_path\": \"/content/insuranceFraud_Dataset.csv\",\n",
        "    \"target_column\": \"fraud_reported\",\n",
        "    \"columns_to_drop\": [\n",
        "        \"months_as_customer\", \"policy_number\", \"insured_zip\", \"insured_hobbies\",\n",
        "        \"incident_date\", \"incident_location\", \"total_claim_amount\",\n",
        "        \"auto_model\", \"incident_city\", \"umbrella_limit\"\n",
        "    ],\n",
        "    \"ordinal_mappings\": {\n",
        "        \"policy_csl\": [\"500/1000\", \"250/500\", \"100/300\"],\n",
        "        \"policy_deductable\": [2000, 1000, 500],\n",
        "        \"insured_education_level\": [\n",
        "            \"PhD\", \"MD\", \"JD\", \"Masters\", \"College\", \"Associate\", \"High School\"\n",
        "        ],\n",
        "        \"incident_severity\": [\n",
        "            \"Total Loss\", \"Major Damage\", \"Minor Damage\", \"Trivial Damage\"\n",
        "        ],\n",
        "        \"number_of_vehicles_involved\": [4, 3, 2, 1],\n",
        "        \"bodily_injuries\": [2, 1, 0],\n",
        "        \"witnesses\": [3, 2, 1, 0],\n",
        "        \"auto_year\": list(range(2015, 1994, -1))\n",
        "    },\n",
        "    \"preprocessor_output_path\": \"/content/preprocessor.pkl\"\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "1StD-12c-aAb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Utility Functions ==========\n",
        "\n",
        "def load_data(path: str) -> pd.DataFrame:\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"File not found: {path}\")\n",
        "    logging.info(\"Loading dataset...\")\n",
        "    return pd.read_csv(path, na_values=[\"?\"])\n",
        "\n",
        "def split_data(df: pd.DataFrame, target: str, test_size=0.25, random_state=42):\n",
        "    X = df.drop(target, axis=1)\n",
        "    y = df[target]\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "def save_pkl(file_path: str, obj: object) -> None:\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            pickle.dump(obj, f)\n",
        "        logging.info(f\"Saved pickle file to: {file_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving {file_path}: {e}\")\n",
        "\n",
        "def extract_year(X):\n",
        "    X = pd.to_datetime(X.squeeze())\n",
        "    return X.dt.year.to_frame()\n",
        "\n",
        "def hour_to_period(X):\n",
        "    hour = X.squeeze()\n",
        "    if hasattr(hour, \"values\"):\n",
        "        hour = hour.values\n",
        "    bins = [0, 6, 12, 18, 24]\n",
        "    labels = [\"Night\", \"Morning\", \"Afternoon\", \"Evening\"]\n",
        "    return pd.DataFrame(pd.cut(hour, bins=bins, labels=labels, right=False, include_lowest=True))\n",
        "\n"
      ],
      "metadata": {
        "id": "dkiTKDkW-kCB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Custom Transformers ==========\n",
        "\n",
        "class KBinsDiscretizerPlusOne(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_bins=5, encode=\"ordinal\", strategy=\"uniform\"):\n",
        "        self.n_bins = n_bins\n",
        "        self.encode = encode\n",
        "        self.strategy = strategy\n",
        "        self.kbd = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.kbd = KBinsDiscretizer(n_bins=self.n_bins, encode=self.encode, strategy=self.strategy)\n",
        "        self.kbd.fit(X)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return (self.kbd.transform(X) + 1).astype(int)\n",
        "\n",
        "class RandomSampleImputer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, random_state=None):\n",
        "        self.random_state = random_state\n",
        "        self.feature_values_ = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = pd.DataFrame(X)\n",
        "        for col in X.columns:\n",
        "            self.feature_values_[col] = X[col].dropna().values\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = pd.DataFrame(X).copy()\n",
        "        rng = np.random.default_rng(self.random_state)\n",
        "        for col in X.columns:\n",
        "            mask = X[col].isna()\n",
        "            if mask.any() and col in self.feature_values_:\n",
        "                X.loc[mask, col] = rng.choice(self.feature_values_[col], size=mask.sum())\n",
        "        return X.values\n",
        "\n"
      ],
      "metadata": {
        "id": "umoSWKpa-j9o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Main Pipeline ==========\n",
        "\n",
        "def build_pipeline(X: pd.DataFrame) -> Pipeline:\n",
        "    categorical_columns = X.select_dtypes(include=[\"object\"]).columns\n",
        "    numerical_columns = X.select_dtypes(exclude=[\"object\"]).columns\n",
        "\n",
        "    ordinal_columns = list(CONFIG[\"ordinal_mappings\"].keys())\n",
        "    ordinal_categories = [CONFIG[\"ordinal_mappings\"][col] for col in ordinal_columns]\n",
        "\n",
        "    nominal_columns = [col for col in categorical_columns if col not in ordinal_columns and col != \"policy_bind_date\"]\n",
        "\n",
        "    date_pipeline = Pipeline([\n",
        "        (\"extract_year\", FunctionTransformer(extract_year)),\n",
        "        (\"bin_year\", KBinsDiscretizerPlusOne()),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "    num_pipeline = Pipeline([\n",
        "        (\"imputer\", IterativeImputer(max_iter=100, random_state=42)),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "    ordinal_pipeline = Pipeline([\n",
        "        (\"imputer\", RandomSampleImputer()),\n",
        "        (\"ordinal_encoder\", OrdinalEncoder(categories=ordinal_categories)),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "    nominal_pipeline = Pipeline([\n",
        "        (\"imputer\", RandomSampleImputer()),\n",
        "        (\"one_hot\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\", sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    hour_pipeline = Pipeline([\n",
        "        (\"hour_to_period\", FunctionTransformer(hour_to_period, validate=False)),\n",
        "        (\"one_hot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer([\n",
        "        (\"date\", date_pipeline, [\"policy_bind_date\"]),\n",
        "        (\"drop_columns\", \"drop\", CONFIG[\"columns_to_drop\"]),\n",
        "        (\"hour\", hour_pipeline, [\"incident_hour_of_the_day\"]),\n",
        "        (\"numerical\", num_pipeline, numerical_columns),\n",
        "        (\"ordinal\", ordinal_pipeline, ordinal_columns),\n",
        "        (\"nominal\", nominal_pipeline, nominal_columns)\n",
        "    ], remainder=\"passthrough\")\n",
        "\n",
        "    return Pipeline([(\"preprocessor\", preprocessor)])\n",
        "\n"
      ],
      "metadata": {
        "id": "Ranm86ni-j5Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Main Execution ==========\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        df = load_data(CONFIG[\"data_path\"])\n",
        "        X_train, X_test, y_train, y_test = split_data(df, CONFIG[\"target_column\"])\n",
        "\n",
        "        pipeline = build_pipeline(X_train)\n",
        "        logging.info(\"Fitting preprocessing pipeline...\")\n",
        "        X_train_arr = pipeline.fit_transform(X_train)\n",
        "        X_test_arr = pipeline.transform(X_test)\n",
        "\n",
        "        train_arr = np.c_[X_train_arr, y_train.values.reshape(-1, 1)]\n",
        "        test_arr = np.c_[X_test_arr, y_test.values.reshape(-1, 1)]\n",
        "\n",
        "        return train_arr, test_arr, pipeline\n",
        "        save_pkl(CONFIG[\"preprocessor_output_path\"], pipeline.named_steps[\"preprocessor\"])\n",
        "        logging.info(\"Preprocessing complete.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Pipeline failed: {e}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gBDBIjaI-j08"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a, b, _= main()"
      ],
      "metadata": {
        "id": "N2odS4K3CYAM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from sklearn.linear_model import (\n",
        "    LogisticRegression, SGDClassifier\n",
        ")\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    AdaBoostClassifier,\n",
        "    BaggingClassifier\n",
        ")\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "def save_pkl(file_path, obj):\n",
        "    \"\"\"Save an object to a pickle file\"\"\"\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        pickle.dump(obj, f)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-PPkFbTPBHx7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_models(X_train, y_train, X_test, y_test, models, param_grids):\n",
        "    \"\"\"\n",
        "    Uses GridSearchCV to perform hyperparameter tuning and evaluation.\n",
        "    Returns dictionary of test accuracies.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        report = {}\n",
        "        for model_name, model_class in models.items():\n",
        "            model = model_class()\n",
        "            params = param_grids.get(model_name, {})\n",
        "            gs = GridSearchCV(model, param_grid=params, cv=3)\n",
        "            gs.fit(X_train, y_train)\n",
        "\n",
        "            best_model = gs.best_estimator_\n",
        "            y_test_pred = best_model.predict(X_test)\n",
        "            test_score = accuracy_score(y_test, y_test_pred)\n",
        "            report[model_name] = test_score\n",
        "\n",
        "        return report\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in evaluating models\", exc_info=True)\n",
        "        raise e\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vV08Oa_9BTmk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initiate_model_trainer(train_arr, test_arr):\n",
        "    try:\n",
        "        # Split features and labels\n",
        "        X_train, y_train = train_arr[:, :-1], train_arr[:, -1]\n",
        "        X_test, y_test = test_arr[:, :-1], test_arr[:, -1]\n",
        "\n",
        "        # Label encoding for classification targets\n",
        "        le = LabelEncoder()\n",
        "        y_train = le.fit_transform(y_train)\n",
        "        y_test = le.transform(y_test)\n",
        "\n",
        "        # Define models and hyperparameter grids\n",
        "        models_dict = {\n",
        "            \"LogisticRegression\": LogisticRegression,\n",
        "            \"SVC\": SVC,\n",
        "            \"KNeighborsClassifier\": KNeighborsClassifier,\n",
        "            \"DecisionTreeClassifier\": DecisionTreeClassifier,\n",
        "            \"RandomForestClassifier\": RandomForestClassifier,\n",
        "            \"GradientBoostingClassifier\": GradientBoostingClassifier,\n",
        "            \"XGBClassifier\": xgb.XGBClassifier,\n",
        "            \"GaussianNB\": GaussianNB,\n",
        "            \"AdaBoostClassifier\": AdaBoostClassifier,\n",
        "            \"BaggingClassifier\": BaggingClassifier,\n",
        "            \"SGDClassifier\": SGDClassifier\n",
        "        }\n",
        "\n",
        "        # Parameter grids (only active parameters shown; you can uncomment others as needed)\n",
        "        param_grids = {\n",
        "            \"LogisticRegression\": {\n",
        "                \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
        "                # \"C\": [0.01, 0.1, 1, 10, 100],\n",
        "                # \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n",
        "                # \"max_iter\": [100, 200, 500]\n",
        "            },\n",
        "            \"SVC\": {\n",
        "                \"C\": [0.1, 1, 10, 100],\n",
        "                # \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
        "                # \"gamma\": [\"scale\", \"auto\"] + [0.001, 0.01, 0.1, 1],\n",
        "                # \"degree\": [2, 3, 4]\n",
        "            },\n",
        "            \"KNeighborsClassifier\": {\n",
        "                \"n_neighbors\": [3, 5, 7, 9, 11],\n",
        "                # \"weights\": [\"uniform\", \"distance\"],\n",
        "                # \"p\": [1, 2]\n",
        "            },\n",
        "            \"DecisionTreeClassifier\": {\n",
        "                \"max_depth\": [None, 5, 10, 20, 30],\n",
        "                # \"min_samples_split\": [2, 5, 10],\n",
        "                # \"min_samples_leaf\": [1, 2, 4],\n",
        "                # \"criterion\": [\"gini\", \"entropy\"]\n",
        "            },\n",
        "            \"RandomForestClassifier\": {\n",
        "                \"n_estimators\": [50, 100, 200],\n",
        "                # \"max_depth\": [None, 10, 20, 30],\n",
        "                # \"min_samples_split\": [2, 5, 10],\n",
        "                # \"min_samples_leaf\": [1, 2, 4],\n",
        "                # \"max_features\": [\"sqrt\", \"log2\"]\n",
        "            },\n",
        "            \"GradientBoostingClassifier\": {\n",
        "                \"n_estimators\": [50, 100, 200],\n",
        "                # \"learning_rate\": [0.01, 0.1, 0.2],\n",
        "                # \"max_depth\": [3, 5, 7],\n",
        "                # \"min_samples_split\": [2, 5, 10],\n",
        "                # \"min_samples_leaf\": [1, 2, 4],\n",
        "                # \"max_features\": [\"sqrt\", \"log2\"],\n",
        "                # \"subsample\": [0.8, 0.9, 1.0]\n",
        "            },\n",
        "            \"XGBClassifier\": {\n",
        "                \"n_estimators\": [50, 100, 200],\n",
        "                # \"learning_rate\": [0.01, 0.1, 0.2],\n",
        "                # \"max_depth\": [3, 5, 7],\n",
        "                # \"min_child_weight\": [1, 3, 5],\n",
        "                # \"subsample\": [0.8, 0.9, 1.0],\n",
        "                # \"colsample_bytree\": [0.8, 0.9, 1.0],\n",
        "                # \"gamma\": [0, 0.1, 0.2],\n",
        "                # \"reg_alpha\": [0, 0.1, 0.5],\n",
        "                # \"reg_lambda\": [0, 0.1, 0.5]\n",
        "            },\n",
        "            \"GaussianNB\": {\n",
        "                \"var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
        "            },\n",
        "\n",
        "            \"AdaBoostClassifier\": {\n",
        "                \"n_estimators\": [50, 100, 200],\n",
        "                # \"learning_rate\": [0.01, 0.1, 0.5],\n",
        "                # \"base_estimator\": [None, DecisionTreeClassifier(max_depth=1)]\n",
        "            },\n",
        "            \"BaggingClassifier\": {\n",
        "                \"n_estimators\": [10, 50, 100],\n",
        "                # \"max_samples\": [0.5, 0.7, 1.0],\n",
        "                # \"max_features\": [0.5, 0.7, 1.0],\n",
        "                # \"base_estimator\": [None, DecisionTreeClassifier(max_depth=2)]\n",
        "            },\n",
        "            \"SGDClassifier\": {\n",
        "                \"loss\": [\"hinge\", \"log_loss\", \"modified_huber\"],\n",
        "                # \"penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
        "                # \"alpha\": [0.0001, 0.001, 0.01],\n",
        "                # \"l1_ratio\": [0, 0.15, 0.5, 1.0],\n",
        "                # \"max_iter\": [1000, 2000],\n",
        "                # \"learning_rate\": [\"constant\", \"optimal\", \"invscaling\"],\n",
        "                # \"eta0\": [0.01, 0.1]\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "        # Evaluate and select best model\n",
        "        model_report = evaluate_models(X_train, y_train, X_test, y_test, models_dict, param_grids)\n",
        "        best_score = max(model_report.values())\n",
        "        best_model_name = max(model_report, key=model_report.get)\n",
        "        best_model = models_dict[best_model_name]()\n",
        "\n",
        "        if best_score < 0.6:\n",
        "            raise Exception(\"No suitable model found with accuracy >= 0.6\")\n",
        "\n",
        "        logging.info(f\"Best Model: {best_model_name} with Accuracy: {best_score}\")\n",
        "        save_pkl(\"/content/best_model.pkl\", best_model)\n",
        "\n",
        "        print(\"Best Model Name:\", best_model_name)\n",
        "        print(\"Accuracy Score:\", best_score)\n",
        "        print(\"All Model Scores:\", model_report)\n",
        "\n",
        "        return best_model_name, best_score, model_report\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in model training pipeline\", exc_info=True)\n",
        "        raise e\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NfVjny9RBThV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "# Replace train_arr and test_arr with your numpy arrays\n",
        "abc = initiate_model_trainer(a, b)\n",
        "print(abc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik6nggSxByPP",
        "outputId": "ee878bc9-cdc2-4d91-ff87-ac6fcc94206f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Name: BaggingClassifier\n",
            "Accuracy Score: 0.808\n",
            "All Model Scores: {'LogisticRegression': 0.716, 'SVC': 0.732, 'KNeighborsClassifier': 0.716, 'DecisionTreeClassifier': 0.788, 'RandomForestClassifier': 0.732, 'GradientBoostingClassifier': 0.792, 'XGBClassifier': 0.776, 'GaussianNB': 0.268, 'AdaBoostClassifier': 0.772, 'BaggingClassifier': 0.808, 'SGDClassifier': 0.696}\n",
            "('BaggingClassifier', 0.808, {'LogisticRegression': 0.716, 'SVC': 0.732, 'KNeighborsClassifier': 0.716, 'DecisionTreeClassifier': 0.788, 'RandomForestClassifier': 0.732, 'GradientBoostingClassifier': 0.792, 'XGBClassifier': 0.776, 'GaussianNB': 0.268, 'AdaBoostClassifier': 0.772, 'BaggingClassifier': 0.808, 'SGDClassifier': 0.696})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Y9VJBJrB2Gl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}