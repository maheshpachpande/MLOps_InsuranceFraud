{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y15gGppHhRBu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Read the Data\n",
        "df = pd.read_csv(\"/content/insuranceFraud_Dataset.csv\", na_values=[\"?\"])\n",
        "\n",
        "# Split data\n",
        "# Prevention for data leakage\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_set, X_test_set = train_test_split(df, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "# Assign the target column\n",
        "target_column = \"fraud_reported\"\n",
        "\n",
        "# Training set\n",
        "X_train_df = X_train_set.drop(target_column, axis=1)\n",
        "\n",
        "# Training target column\n",
        "y_train_df = X_train_set[target_column]\n",
        "\n",
        "# Testing set\n",
        "X_test_df = X_test_set.drop(target_column, axis=1)\n",
        "\n",
        "# Testing target column\n",
        "y_test_df = X_test_set[target_column]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical Columns in Training set\n",
        "categorical_columns = X_train_df.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "# Numerical columns in Training set\n",
        "numerical_columns = X_train_df.select_dtypes(exclude=[\"object\"]).columns\n",
        "\n",
        "# Define ordinal columns\n",
        "ordinal_columns = [\n",
        "    \"policy_csl\",\n",
        "    \"policy_deductable\",\n",
        "    \"insured_education_level\",\n",
        "    \"incident_severity\",\n",
        "    \"number_of_vehicles_involved\",\n",
        "    \"bodily_injuries\",\n",
        "    \"witnesses\",\n",
        "    \"auto_year\",\n",
        "]\n",
        "\n",
        "# Define categories for ordinal encoding\n",
        "policy_csl = [\"500/1000\", \"250/500\", \"100/300\"]\n",
        "policy_deductable = [2000, 1000, 500]\n",
        "insured_education_level = [\n",
        "    \"PhD\",\n",
        "    \"MD\",\n",
        "    \"JD\",\n",
        "    \"Masters\",\n",
        "    \"College\",\n",
        "    \"Associate\",\n",
        "    \"High School\",\n",
        "]\n",
        "incident_severity = [\n",
        "    \"Total Loss\",\n",
        "    \"Major Damage\",\n",
        "    \"Minor Damage\",\n",
        "    \"Trivial Damage\",\n",
        "]\n",
        "number_of_vehicles_involved = [4, 3, 2, 1]\n",
        "bodily_injuries = [2, 1, 0]\n",
        "witnesses = [3, 2, 1, 0]\n",
        "auto_year = [\n",
        "    2015,\n",
        "    2014,\n",
        "    2013,\n",
        "    2012,\n",
        "    2011,\n",
        "    2010,\n",
        "    2009,\n",
        "    2008,\n",
        "    2007,\n",
        "    2006,\n",
        "    2005,\n",
        "    2004,\n",
        "    2003,\n",
        "    2002,\n",
        "    2001,\n",
        "    2000,\n",
        "    1999,\n",
        "    1998,\n",
        "    1997,\n",
        "    1996,\n",
        "    1995,\n",
        "]\n",
        "\n",
        "# Define the Nominal columns\n",
        "nominal_categorical_columns = [\n",
        "    col\n",
        "    for col in categorical_columns\n",
        "    if col not in ordinal_columns and col != \"policy_bind_date\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "Yv_IGLhXhgus"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns to Drop\n",
        "columns_to_drop = [\n",
        "    \"months_as_customer\",\n",
        "    \"policy_number\",\n",
        "    \"insured_zip\",\n",
        "    \"insured_hobbies\",\n",
        "    \"incident_date\",\n",
        "    \"incident_location\",\n",
        "    \"total_claim_amount\",\n",
        "    \"auto_model\",\n",
        "    \"incident_city\",\n",
        "    \"umbrella_limit\",\n",
        "]\n"
      ],
      "metadata": {
        "id": "7G7aX6VzjQ1P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fraction_to_float(x):\n",
        "    # Check if input is a string and contains a slash (indicating a fraction)\n",
        "    if isinstance(x, str) and \"/\" in x:\n",
        "        # Split the string into numerator and denominator, convert both to float\n",
        "        num, denom = map(float, x.split(\"/\"))\n",
        "        # Return the result of dividing numerator by denominator\n",
        "        return num / denom\n",
        "    # If not a fraction, try to convert directly to float\n",
        "    try:\n",
        "        return float(x)\n",
        "    # If conversion fails (e.g., invalid string or wrong type), return NaN\n",
        "    except (ValueError, TypeError):\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "def extract_year(X):\n",
        "    X = pd.to_datetime(X.squeeze())\n",
        "    return X.dt.year.to_frame()\n",
        "\n",
        "\n",
        "def hour_to_period(X):\n",
        "    hour = X.squeeze()\n",
        "    if hasattr(hour, \"values\"):\n",
        "        hour = hour.values\n",
        "    bins = [0, 6, 12, 18, 24]\n",
        "    labels = [\"Night\", \"Morning\", \"Afternoon\", \"Evening\"]\n",
        "    return pd.DataFrame(\n",
        "        pd.cut(\n",
        "            hour, bins=bins, labels=labels, right=False, include_lowest=True\n",
        "        )\n",
        "    )\n"
      ],
      "metadata": {
        "id": "34IUBRqYnlvV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import (\n",
        "    FunctionTransformer,\n",
        "    KBinsDiscretizer,\n",
        "    StandardScaler,\n",
        "    OneHotEncoder,\n",
        "    OrdinalEncoder,\n",
        ")\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class KBinsDiscretizerPlusOne(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom discretizer that adds 1 to KBinsDiscretizer output.\n",
        "    Converts continuous features into discrete bins with 1-based indexing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_bins=5, encode=\"ordinal\", strategy=\"uniform\"):\n",
        "        self.n_bins = n_bins\n",
        "        self.encode = encode\n",
        "        self.strategy = strategy\n",
        "        self.kbd = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the discretizer to the data.\"\"\"\n",
        "        self.kbd = KBinsDiscretizer(\n",
        "            n_bins=self.n_bins, encode=self.encode, strategy=self.strategy\n",
        "        )\n",
        "        self.kbd.fit(X)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform data by discretizing and adding 1 to each bin.\"\"\"\n",
        "        return (self.kbd.transform(X) + 1).astype(int)\n",
        "\n",
        "    def get_feature_names_out(self, input_features=None):\n",
        "        \"\"\"Return feature names for transformed output.\"\"\"\n",
        "        if input_features is None:\n",
        "            return [\n",
        "                f\"feature_{i}_binned\" for i in range(self.kbd.n_features_in_)\n",
        "            ]\n",
        "        return [f\"{feature}_binned\" for feature in input_features]\n",
        "\n",
        "\n",
        "class RandomSampleImputer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Custom imputer that fills missing values by randomly sampling\n",
        "    from existing non-null values in each column.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, random_state=None):\n",
        "        self.random_state = random_state\n",
        "        self.feature_values_ = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Learn the non-null values for each feature.\"\"\"\n",
        "        X = pd.DataFrame(X)\n",
        "        for col in X.columns:\n",
        "            non_null = X[col].dropna()\n",
        "            self.feature_values_[col] = non_null.values\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Fill missing values with random samples from non-null values.\"\"\"\n",
        "        X = pd.DataFrame(X).copy()\n",
        "        rng = np.random.default_rng(self.random_state)\n",
        "\n",
        "        for col in X.columns:\n",
        "            mask = X[col].isna()\n",
        "            if mask.any() and col in self.feature_values_:\n",
        "                samples = rng.choice(\n",
        "                    self.feature_values_[col], size=mask.sum()\n",
        "                )\n",
        "                X.loc[mask, col] = samples\n",
        "\n",
        "        return X.values\n"
      ],
      "metadata": {
        "id": "Fn5YJtgilLS5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rkbPy3b-niBL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracts the year from date fields using a custom function.\n",
        "# Discretizes the extracted years into bins and increments them by 1 (custom transformer).\n",
        "\n",
        "date_pipeline = Pipeline(steps=[\n",
        "    (\"extract_year\", FunctionTransformer(extract_year)),\n",
        "    (\"bin_year\", KBinsDiscretizerPlusOne()),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "])\n",
        "\n",
        "# Fills missing numerical values using multivariate iterative imputation (like chained regression)\n",
        "# Standardizes numerical values to mean 0 and standard deviation 1.\n",
        "\n",
        "num_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", IterativeImputer(max_iter=100, random_state=42)),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Fills missing categorical values by randomly sampling from non-missing values.\n",
        "# Converts ordered categorical variables into integer-encoded form, preserving order.\n",
        "\n",
        "ordinal_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", RandomSampleImputer()),\n",
        "        (\n",
        "            \"ordinal_encoder\",\n",
        "            OrdinalEncoder(\n",
        "                categories=[\n",
        "                    policy_csl,\n",
        "                    policy_deductable,\n",
        "                    insured_education_level,\n",
        "                    incident_severity,\n",
        "                    number_of_vehicles_involved,\n",
        "                    bodily_injuries,\n",
        "                    witnesses,\n",
        "                    auto_year,\n",
        "                ]\n",
        "            ),\n",
        "        ),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Handles missing nominal (unordered categorical) values via random sampling.\n",
        "# Applies one-hot encoding (excluding the first category to avoid multicollinearity), while ignoring unknown categories.\n",
        "\n",
        "nominal_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", RandomSampleImputer()),\n",
        "        (\n",
        "            \"one_hot\",\n",
        "            OneHotEncoder(\n",
        "                handle_unknown=\"ignore\", drop=\"first\", sparse_output=False\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "hour_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        (\n",
        "            \"hour_to_period\",\n",
        "            FunctionTransformer(hour_to_period, validate=False),\n",
        "        ),\n",
        "        (\n",
        "            \"one_hot\",\n",
        "            OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"),\n",
        "        ),\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "jXNsq7uIlMPX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"date\", date_pipeline, [\"policy_bind_date\"]),\n",
        "        ('drop_columns', 'drop', columns_to_drop),\n",
        "        (\"hour\", hour_pipeline, [\"incident_hour_of_the_day\"]),\n",
        "        ('numerical', num_pipeline, numerical_columns),\n",
        "        ('ordinal', ordinal_pipeline, ordinal_columns),\n",
        "        ('nominal', nominal_pipeline, nominal_categorical_columns)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")"
      ],
      "metadata": {
        "id": "_47e9v-5nNDu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([(\"preprocessor\", preprocessor)])\n",
        "\n",
        "# Fit and transform\n",
        "X_train_df_arr = pipeline.fit_transform(X_train_df)\n",
        "X_test_df_arr = pipeline.transform(X_test_df)\n"
      ],
      "metadata": {
        "id": "1enUZ2Dbp4iE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_arr = np.c_[X_train_df_arr, y_train_df.values.reshape(-1, 1)]\n",
        "test_arr = np.c_[X_test_df_arr, y_test_df.values.reshape(-1, 1)]"
      ],
      "metadata": {
        "id": "nz9TaQerp-Ev"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "def save_pkl(file_path, obj):\n",
        "  try:\n",
        "    dir_path = os.path.dirname(file_path)\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "    with open(file_path, 'wb') as f:\n",
        "      pickle.dump(obj, f)\n",
        "  except Exception as e:\n",
        "    print(f\"Error saving {file_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "nbZV1IXkvdGc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_pkl(\"/content/preprocessor.pkl\", preprocessor)"
      ],
      "metadata": {
        "id": "4Xx9M89AwZlK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Trainer"
      ],
      "metadata": {
        "id": "zfo-rMxHHh4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "pX_uE754wfAd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def evaluate_models(X_train, y_train, X_test, y_test, models, param_grids):\n",
        "    \"\"\"\n",
        "    Uses GridSearchCV to perform hyperparameter tuning for each model in a provided dictionary,\n",
        "    selecting the best parameters and evaluating model performance on both training and test sets.\n",
        "    It returns a report of each model’s test accuracy after tuning.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        report = {}\n",
        "        for model_name in models.keys():\n",
        "\n",
        "            # Get the model from dictionary\n",
        "            model_class = models[model_name]\n",
        "\n",
        "            # Instantiate the model\n",
        "            model = model_class()\n",
        "            # Get the parameter grid for this model\n",
        "            para = param_grids[model_name]\n",
        "            # Set up GridSearchCV\n",
        "            gs = GridSearchCV(model, param_grid=para, cv=3)\n",
        "            gs.fit(X_train, y_train)\n",
        "\n",
        "            # Get the best model with best parameters\n",
        "            best_model = gs.best_estimator_\n",
        "            # Make predictions\n",
        "            y_test_pred = best_model.predict(X_test)\n",
        "            # Calculate test accuracy\n",
        "            test_model_score = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "            # Append to report\n",
        "            report[model_name] = test_model_score\n",
        "\n",
        "        return report\n",
        "        print(report)\n",
        "\n",
        "    except Exception as e:\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "Rg-9RbjjKfAn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    AdaBoostClassifier,\n",
        "    BaggingClassifier\n",
        ")\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "def initiate_model_trainer(train_arr, test_arr):\n",
        "    try:\n",
        "        # Split Data\n",
        "        X_train, y_train = train_arr[:, :-1], train_arr[:, -1]\n",
        "        X_test, y_test = test_arr[:, :-1], test_arr[:, -1]\n",
        "\n",
        "        # Assuming y_train and y_test are your target arrays\n",
        "        le = LabelEncoder()\n",
        "        y_train = le.fit_transform(y_train)\n",
        "        y_test = le.transform(y_test)\n",
        "\n",
        "        # Models\n",
        "        models_dict = {\n",
        "            \"LogisticRegression\": LogisticRegression,\n",
        "            \"SVC\": SVC,\n",
        "            \"KNeighborsClassifier\": KNeighborsClassifier,\n",
        "            \"DecisionTreeClassifier\": DecisionTreeClassifier,\n",
        "            \"RandomForestClassifier\": RandomForestClassifier,\n",
        "            \"GradientBoostingClassifier\": GradientBoostingClassifier,\n",
        "            \"XGBClassifier\": xgb.XGBClassifier,\n",
        "            \"GaussianNB\": GaussianNB,\n",
        "            \"AdaBoostClassifier\": AdaBoostClassifier,\n",
        "            \"BaggingClassifier\": BaggingClassifier,\n",
        "            \"SGDClassifier\": SGDClassifier\n",
        "        }\n",
        "\n",
        "        # Parameter grids (only active parameters shown; you can uncomment others as needed)\n",
        "        param_grids = {\n",
        "            \"LogisticRegression\": {\n",
        "                \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
        "                # \"C\": [0.01, 0.1, 1, 10, 100],\n",
        "                # \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n",
        "                # \"max_iter\": [100, 200, 500]\n",
        "            },\n",
        "            \"SVC\": {\n",
        "                \"C\": [0.1, 1, 10, 100],\n",
        "                # \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
        "                # \"gamma\": [\"scale\", \"auto\"] + [0.001, 0.01, 0.1, 1],\n",
        "                # \"degree\": [2, 3, 4]\n",
        "            },\n",
        "            \"KNeighborsClassifier\": {\n",
        "                \"n_neighbors\": [3, 5, 7, 9, 11],\n",
        "                # \"weights\": [\"uniform\", \"distance\"],\n",
        "                # \"p\": [1, 2]\n",
        "            },\n",
        "            \"DecisionTreeClassifier\": {\n",
        "                \"max_depth\": [None, 5, 10, 20, 30],\n",
        "                # \"min_samples_split\": [2, 5, 10],\n",
        "                # \"min_samples_leaf\": [1, 2, 4],\n",
        "                # \"criterion\": [\"gini\", \"entropy\"]\n",
        "            },\n",
        "            \"RandomForestClassifier\": {\n",
        "                \"n_estimators\": [50, 100, 200],\n",
        "                # \"max_depth\": [None, 10, 20, 30],\n",
        "                # \"min_samples_split\": [2, 5, 10],\n",
        "                # \"min_samples_leaf\": [1, 2, 4],\n",
        "                # \"max_features\": [\"sqrt\", \"log2\"]\n",
        "            },\n",
        "            \"GradientBoostingClassifier\": {\n",
        "                \"n_estimators\": [50, 100, 200],\n",
        "                # \"learning_rate\": [0.01, 0.1, 0.2],\n",
        "                # \"max_depth\": [3, 5, 7],\n",
        "                # \"min_samples_split\": [2, 5, 10],\n",
        "                # \"min_samples_leaf\": [1, 2, 4],\n",
        "                # \"max_features\": [\"sqrt\", \"log2\"],\n",
        "                # \"subsample\": [0.8, 0.9, 1.0]\n",
        "            },\n",
        "            \"XGBClassifier\": {\n",
        "                \"n_estimators\": [50, 100, 200],\n",
        "                # \"learning_rate\": [0.01, 0.1, 0.2],\n",
        "                # \"max_depth\": [3, 5, 7],\n",
        "                # \"min_child_weight\": [1, 3, 5],\n",
        "                # \"subsample\": [0.8, 0.9, 1.0],\n",
        "                # \"colsample_bytree\": [0.8, 0.9, 1.0],\n",
        "                # \"gamma\": [0, 0.1, 0.2],\n",
        "                # \"reg_alpha\": [0, 0.1, 0.5],\n",
        "                # \"reg_lambda\": [0, 0.1, 0.5]\n",
        "            },\n",
        "            \"GaussianNB\": {\n",
        "                \"var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
        "            },\n",
        "\n",
        "            \"AdaBoostClassifier\": {\n",
        "                \"n_estimators\": [50, 100, 200],\n",
        "                # \"learning_rate\": [0.01, 0.1, 0.5],\n",
        "                # \"base_estimator\": [None, DecisionTreeClassifier(max_depth=1)]\n",
        "            },\n",
        "            \"BaggingClassifier\": {\n",
        "                \"n_estimators\": [10, 50, 100],\n",
        "                # \"max_samples\": [0.5, 0.7, 1.0],\n",
        "                # \"max_features\": [0.5, 0.7, 1.0],\n",
        "                # \"base_estimator\": [None, DecisionTreeClassifier(max_depth=2)]\n",
        "            },\n",
        "            \"SGDClassifier\": {\n",
        "                \"loss\": [\"hinge\", \"log_loss\", \"modified_huber\"],\n",
        "                # \"penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
        "                # \"alpha\": [0.0001, 0.001, 0.01],\n",
        "                # \"l1_ratio\": [0, 0.15, 0.5, 1.0],\n",
        "                # \"max_iter\": [1000, 2000],\n",
        "                # \"learning_rate\": [\"constant\", \"optimal\", \"invscaling\"],\n",
        "                # \"eta0\": [0.01, 0.1]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Now use models_dict and param_grids for model training and tuning\n",
        "        model_report: dict=evaluate_models(X_train, y_train, X_test, y_test, models_dict, param_grids)\n",
        "\n",
        "        # Get the best model\n",
        "        best_model_score = max(sorted(model_report.values()))\n",
        "\n",
        "        # Get the best model name\n",
        "        best_model_name = list(model_report.keys())[\n",
        "            list(model_report.values()).index(best_model_score)\n",
        "        ]\n",
        "        best_model = models_dict[best_model_name]\n",
        "\n",
        "        # Model Threshould\n",
        "        if best_model_score < 0.6:\n",
        "            raise (\"No best model found\")\n",
        "        logging.info(\"Best found model on both training and testing dataset\")\n",
        "\n",
        "        save_pkl(\"/content/best_model.pkl\", best_model)\n",
        "        print(best_model_score)\n",
        "        print(best_model_name)\n",
        "        print( model_report)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        raise e\n"
      ],
      "metadata": {
        "id": "MBC3KF3xKlIs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abc = initiate_model_trainer(train_arr, test_arr)\n",
        "print(abc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTkyPBurPmTJ",
        "outputId": "a4c93fb1-5d94-4585-863c-71980b4b5881"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.808\n",
            "BaggingClassifier\n",
            "{'LogisticRegression': 0.72, 'SVC': 0.732, 'KNeighborsClassifier': 0.708, 'DecisionTreeClassifier': 0.792, 'RandomForestClassifier': 0.728, 'GradientBoostingClassifier': 0.8, 'XGBClassifier': 0.784, 'GaussianNB': 0.268, 'AdaBoostClassifier': 0.752, 'BaggingClassifier': 0.808, 'SGDClassifier': 0.66}\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YBbE1EUUUghr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uxY5qz2Mbk8J"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}